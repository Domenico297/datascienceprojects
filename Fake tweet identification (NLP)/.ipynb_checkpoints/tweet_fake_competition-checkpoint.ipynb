{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import data and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 1000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TXT_abc</th>\n",
       "      <th>TXT_abc news</th>\n",
       "      <th>TXT_ablaze</th>\n",
       "      <th>TXT_accident</th>\n",
       "      <th>TXT_action</th>\n",
       "      <th>TXT_actually</th>\n",
       "      <th>TXT_added</th>\n",
       "      <th>TXT_affected</th>\n",
       "      <th>TXT_affected fatal</th>\n",
       "      <th>TXT_aftershock</th>\n",
       "      <th>...</th>\n",
       "      <th>TXT_years</th>\n",
       "      <th>TXT_yes</th>\n",
       "      <th>TXT_york</th>\n",
       "      <th>TXT_young</th>\n",
       "      <th>TXT_youth</th>\n",
       "      <th>TXT_youtube</th>\n",
       "      <th>TXT_youtube video</th>\n",
       "      <th>TXT_yr</th>\n",
       "      <th>TXT_yr old</th>\n",
       "      <th>TXT_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   TXT_abc  TXT_abc news  TXT_ablaze  TXT_accident  TXT_action  TXT_actually  \\\n",
       "0      0.0           0.0         0.0           0.0         0.0           0.0   \n",
       "1      0.0           0.0         0.0           0.0         0.0           0.0   \n",
       "2      0.0           0.0         0.0           0.0         0.0           0.0   \n",
       "3      0.0           0.0         0.0           0.0         0.0           0.0   \n",
       "4      0.0           0.0         0.0           0.0         0.0           0.0   \n",
       "\n",
       "   TXT_added  TXT_affected  TXT_affected fatal  TXT_aftershock  ...  \\\n",
       "0        0.0           0.0                 0.0             0.0  ...   \n",
       "1        0.0           0.0                 0.0             0.0  ...   \n",
       "2        0.0           0.0                 0.0             0.0  ...   \n",
       "3        0.0           0.0                 0.0             0.0  ...   \n",
       "4        0.0           0.0                 0.0             0.0  ...   \n",
       "\n",
       "   TXT_years  TXT_yes  TXT_york  TXT_young  TXT_youth  TXT_youtube  \\\n",
       "0        0.0      0.0       0.0        0.0        0.0          0.0   \n",
       "1        0.0      0.0       0.0        0.0        0.0          0.0   \n",
       "2        0.0      0.0       0.0        0.0        0.0          0.0   \n",
       "3        0.0      0.0       0.0        0.0        0.0          0.0   \n",
       "4        0.0      0.0       0.0        0.0        0.0          0.0   \n",
       "\n",
       "   TXT_youtube video  TXT_yr  TXT_yr old  TXT_zone  \n",
       "0                0.0     0.0         0.0       0.0  \n",
       "1                0.0     0.0         0.0       0.0  \n",
       "2                0.0     0.0         0.0       0.0  \n",
       "3                0.0     0.0         0.0       0.0  \n",
       "4                0.0     0.0         0.0       0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# cleaning the text\n",
    "train_df['text']=train_df['text'].str.replace('[^a-zA-Z]',' ') \n",
    "\n",
    "\n",
    "\n",
    "# converting the text in vectorial form\n",
    "tf=TfidfVectorizer(stop_words='english',ngram_range=(1,2),max_features=1000)   \n",
    "tf_vec=tf.fit_transform(train_df['text'])\n",
    "\n",
    "# build a new DataFrame with the converted text\n",
    "text_df=pd.DataFrame(tf_vec.toarray(),columns=tf.get_feature_names()).add_prefix('TXT_')\n",
    "\n",
    "print(text_df.shape)\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating the two dataframes\n",
    "new_df=pd.concat([train_df,text_df],axis=1)\n",
    "new_df.drop('text',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing features and targets for the machine learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=new_df['target']\n",
    "X=new_df.drop(['keyword','location','target','id'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------\n",
      "Confusion matrix \n",
      "[[956 284]\n",
      " [130 534]]\n",
      "----------------------------------------------------\n",
      "Report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82      1086\n",
      "           1       0.80      0.65      0.72       818\n",
      "\n",
      "    accuracy                           0.78      1904\n",
      "   macro avg       0.79      0.77      0.77      1904\n",
      "weighted avg       0.79      0.78      0.78      1904\n",
      "\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,stratify=y)\n",
    "\n",
    "rf=MultinomialNB()\n",
    "\n",
    "rf.fit(X_train,y_train)\n",
    "y_pred=rf.predict(X_test)\n",
    "\n",
    "print(\"----------------------------------------------------\") \n",
    "print(\"Confusion matrix \\n\"+str(confusion_matrix(y_pred,y_test))) \n",
    "print(\"----------------------------------------------------\") \n",
    "print(\"Report \\n\"+str(classification_report(y_test,y_pred))) \n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "#crosscv=cross_validate(rf,X,y,cv=5)['test_score'].mean()\n",
    "#print('The cross validation score is '+str(crosscv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(test_df):\n",
    "    \n",
    "    aa=np.array(test_df['id'])\n",
    "    \n",
    "    test_df['text']=test_df['text'].str.replace('[^a-zA-Z]',' ') \n",
    "\n",
    "    # converting the text in tabular form \n",
    "    tf_vec=tf.transform(test_df['text'])\n",
    "\n",
    "    # build a new DataFrame with the converted text\n",
    "    text_df=pd.DataFrame(tf_vec.toarray(),columns=tf.get_feature_names()).add_prefix('TXT_')\n",
    "\n",
    "    # concatenating the 2 data frames\n",
    "    new_df=pd.concat([test_df,text_df],axis=1)\n",
    "    new_df.drop('text',axis=1,inplace=True)\n",
    "\n",
    "    X=new_df.drop(['keyword','location','id'],axis=1)\n",
    "    bb=rf.predict(X)\n",
    "    \n",
    "    return aa,bb,X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final predictions using the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=pd.read_csv('test.csv')\n",
    "\n",
    "peppe=predictions(test_df)\n",
    "\n",
    "risultati=pd.DataFrame(list(zip(peppe[0],peppe[1])),columns=['id','target'])\n",
    "\n",
    "risultati.to_csv('risultati.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
